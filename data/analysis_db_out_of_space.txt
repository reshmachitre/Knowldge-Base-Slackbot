Analysis DB running out of free disk spaceFreeing up space on the Analysis DB instanceOverviewThis runbook guides engineers in responding to alerts indicating that the Target Schema Mapping Analysis DB instance is running out of database space, especially due to blocked partition truncation caused by a concurrent DB-intensive query.Runbook1. Check the AWS RDS instance console to monitor the current free disk space and assess the rate at which it's decreasing.If the storage usage shows a gradual, organic growth, it's likely that the scheduled analysis cleanup job will eventually free up space as expected. However, if there's a sudden spike in storage consumption within the last few hours, this typically indicates an abnormal condition that may require manual intervention.8. Check Table Storage UsageOpen pganalyze and inspect which tables are consuming the most disk space for the DB in this case analysis DB. In most cases, the top offender is: public.target_schema_product_field_analysis_results2 9. Identify the Problematic PartitionDrill down to the specific partition that's hogging space. If the data in that partition is from a completed analysis, and since these results are ephemeral and can be regenerated, it's usually safe to delete the partition.?? Still, confirm the partition isn’t part of an active analysis before deleting from the logs.10. Before deleting the partition, it is probably a good idea to stop the analysis generating the high count of jobs:1. Check Number of Running JobsEvaluate how many jobs are currently running to estimate load on the system. https://onenr.io/0ERPPeGX2RW2. Delayed Job Queue Health: https://onenr.io/0ERz0l5MnQr3. Fetch the org ID associated with the running analysis. Use the Rails console to inspect summary stats:TargetSchemas::MappingAnalysis.running.pluck(:summary_stats) 4. Look for unusually high counts of products or fields, which may explain prolonged or stuck analyses.5. Stop Long-Running Analysis (If Needed) If one analysis has been running for a long time and is likely blocked or stale:mapping_analysis.stop This will also stop the associated DelayedJobs.11. If there's an active query running for multiple hours and blocking other operations, any attempt to truncate will likely timeout as well.To proceed, we should first terminate the process (PID) associated with the blocking query.To kill the PID follow the steps here:1. sal k8s run-psql -n dandelion --context production --custom-db-env-var ANALYSES_DATABASE_URL2. SELECT pg_cancel_backend(PID);3. SELECT pg_terminate_backend(PID);12. Trunacte the table partition - Note: A Rake task for this operation is currently under review. Until it’s available, use the following manual approach. Replace {partition_id} with the actual ID.13. AnalysesRecord.connection.execute("SET statement_timeout = #{10.minutes.in_milliseconds}")14. sql = "TRUNCATE TABLE #{TargetSchemas::ProductFieldAnalysisResult.table_name}_{partition_id};"TargetSchemas::ProductFieldAnalysisResult.connection.execute(sql)15. Monitor Free Storage After truncating the partition, monitor the RDS instance in the AWS Console.Note that it may take a few minutes for the updated free storage metrics to appear