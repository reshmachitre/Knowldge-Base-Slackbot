Salsify's PaaS Primer (Product XM)Salsify Core Engineering’s Infrastructure team builds and maintains a collection of services that we refer to as our PaaS solution for other teams to build upon. This document goes over the components of that PaaS and how you can leverage them to build products and services.ComponentsKubernetes (k8s)Salsify runs two persistent Kubernetes clusters for developer use: production and staging. We also have Test Environments, which launch and configure ephemeral Kubernetes clusters for your use. All resources for a given application are kept in a single Namespace, and those resources are generated using ksonnify.Container orchestration is carried out by Kubernetes. It is responsible to keeping containers running and shuffling them around if there are disruptions in the infrastructure. A cluster consists of control-plane (AKA masters) and worker nodes. The masters schedule containers onto available nodes. The workers are the compute power behind running applications.The masters run in a multi master set up and are quorum based. We keep separate clusters per environment, production, staging, and when needed development. Both production and staging have 3 masters thus a fault tolerance of 1. The development cluster is whatever you need it to be for testing. If you would like to learn more on the internals, visit: Kubernetes.DeploymentsDeployments at Salsify are managed via Infra-mgmt and configured via several customisable files. For more information see Salsify PXM PaaS: Service Deployments.PostgreSQL Databases via RDSMost everything at Salsify uses Postgres for persistence. See Salsify PXM PaaS: RDS Databases for more information.Redis via ElastiCacheMany services use Redis for caching, both as part of our worker strategy and for performance caching. See Salsify PXM PaaS: ElastiCache for more information.Kafka via Managed Streaming for Kafka (MSK)We manage a Kafka cluster for each persistent environment hosted on Amazon MSK. See Managing Cloud Resources With Terraform for how to make changes to these clusters. In test environments, these are deployed in-cluster as as single container image.RabbitMQ via CloudAMQPThis messaging platform predates our adoption of Kafka and should generally not be used for new service development.We manage a hosted RabbitMQ instance for each persistent environment. See Managing Cloud Resources With Terraform for how to make changes to these clusters. Like Kafka, we provide a single container image for this purpose in test environments.Secrets via Secrets APIFor innocuous environment variables such as configuration, use Ksonnify’s deploy.jsonnetfile to add environment variables to your service.Salsify’s PaaS supports two ways to expose sensitive information to instances of your service:1. By setting them as environment variables.2. By storing them in AWS Systems Manager’s Parameter Store (SSM).Each has trade-offs explained below.Environment VariablesEnvironment variables are convenient for sensitive data that changes infrequently. To set these on your service, use our Secrets API service and choose “Environment Variable”.One major drawback to using environment variables is that your application will need to be re-deployed when they are updated. In addition, storing data in the environment runs the risk of malicious or defectively libraries dumping all environment variables to an insecure location, resulting in a security incident.Amazon SSM “Direct Access” SecretsWe refer to these as “Direct Access” secrets as the application loads them directly from the secret store. They can be updated in our Secrets API service.Storing encrypted secrets in Amazon SSM has some specific benefits:* They can be updated immediately without restarting your application.* We can see when they were last updated, and last accessed.* They reduce the risk that a malicious or defective library will dump all environment variables to an insecure location, resulting in a security incident.The main downside associated with using this approach is that the application needs to make decisions about whether to read this data each time it’s needed, or if some kind of caching strategy is required for high-throughput code paths, in order to avoid rate limits and costs associated with hitting the backing store each time you need this data.Postgres DatabasesWe provision and managed RDS database for services that need relational storage. See Salsify PXM PaaS: RDS Databases for more information.MonitoringWe use Prometheus for collecting time-series metrics, Alertmanager for sending notifications on these metrics, and Grafana for visualizing them.Productionhttps://thanos-production.ops.salsify.comhttps://grafana.ops.salsify.comStaginghttps://thanos-staging.ops.salsify.comhttps://grafana-staging.ops.salsify.comWhile you can use the Thanos UI to search for metrics and observe them over time, Grafana does provide the same functionality, without you needing to configure a dashboard. Look into the Explorer tab for a UX similar to that of Thanos, with autocomplete for metrics and parameters, and much more.Logging via NewRelicWe use NewRelic Logs as a hosted solution. See Salsify PXM PaaS: Logging for more information.Salsify PaaS CLIBelow is a list of CLI interfaces to our platform. They are provided by two different tools:* Our in-house sal tool, which abstracts all our Salsify-specific configuration and patterns.* The Kubernetes CLI tool, kubectl. For Kubernetes commands we generally prefer to use the upstream as our usage of Kubernetes is pretty standard.A quick resource for common kubectl commands is the Kubectl Cheatsheet.Commands that require privilege elevation before being run are marked with .The kubectlexamples below use --context / -c and --namespace / -n to set the environment and application name.TaskCommandNotesAuthenticate to Kubernetes via Googlesal k8s authenticateDefaults to reading and writing to ~/.kube/configElevates your session for privileged accesssal k8s elevate -n <app> -r <reason> --context <env>Gives you an 1 hour elevated privilege sessionView list of available application namespaceskubectl get namespaces --selector generator=ksonnify --context <env>View list of running pods and number of restartskubectl get pods -n <app> --context <env>View more information about a podkubectl describe pod <pod-name> -n <app> --context <env>View application logs with podkubectl logs -f <pod-name> -n <app> --context <env>-f will tailView processes and current replica countkubectl get deployments -n <app> --context <env>Temporarily scale a process up or downkubectl scale deployment <deployment-name> -n <app> --context <env> --replicas=<count>If you are running the auto-scaler, it will revert this on its next poll. You might want to scale it down temporarily first if you want this to stick.Stop an application completelysal k8s app stop -n <app> --context <env>Equivalent of setting every deployment to 0Start an application after it has been stoppedsal k8s app start -n <app> --context <env>Brings all deployments back to their count when you ran kubectl app stopRestart an applicationsal k8s app restart -n <app> --context <env>Restarts all running pods and brings them back up gracefully. Leaves one off jobs in placeForce Restart of a process/podkubectl delete pod <pod-name> -n <app> --context <env>Force recreate process/pods, no waiting. Will experience interruption of traffic!Restart a specific deployment typesal k8s app restart -n <app> --context <env> -t <type>Deletes all pods for specific pod-type (web, worker, etc) and bring them back up,Run a command in a consolesal k8s run-cmd -n <app> --context <env> <command + args>Can do bash as well etcAccess rails consolesal k8s run-cmd -n <app> --context <env> -- rails c All commands to run in the console should be after -- as shown hereAccess app databasesal k8s run-psql -n <app> --context <env>Access app custom databasesal k8s run-psql -n <app> --context <env> --custom-db-env-var ENV_VARIABLE_DATABASE_URLList all running pods for an appkubectl get pods -n <app> --context <env>Rollback immediately after deploymentsal k8s app rollback -n <app> --context <env> (--yes)Only use this to rollback immediately after a bad deploy. Performing a second rollback will actually roll forward.List jobskubectl get jobs <job-name> -n <app> --context <env>List and get the status of jobs.Delete jobskubectl delete job <job-name(s)> -n <app> --context <env>Delete a job.