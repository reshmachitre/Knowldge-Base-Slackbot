Elasticsearch Master Node Crash RunbookSymptomsOn a couple occasions, weÕve had a master node in the indexing-service-02 cluster run into an OOM error causing the cluster to go ÒredÓ making the Indexing Service unable to service requests. Some of the ways you can tell this is happening include:* Seeing a loss of metrics for the cluster in Grafana* Seeing readiness probe failed error alert firing: https://onenr.io/0EjO672vAR6* A flood of messages in the cluster's logs about not being able to discover a master* Elasticsearch::Transport::Transport::Errors::ServiceUnavailable errors in the Indexing Service* If you are able to load Salsify, you wonÕt be able to load things like the Product Index Page or searchStepsScale Down the Indexing ServiceLoad from the Indexing Service can inhibit the clusterÕs ability to recover, so we might opt to shut down the web and indexing workers to reduce load on the cluster:kubectl scale deployment indexing-service-auto-scaler --context production --namespace indexing-service --replicas=0kubectl scale deployment indexing-service-web --context production --namespace indexing-service --replicas=0kubectl scale deployment indexing-service-indexing-worker --context production --namespace indexing-service --replicas=0As the Indexing Service fails to handle requests, a circuit breaker in Dandelion should trip which should also help reduce load to the service.Wait for the Cluster to RecoverUnfortunately, there is not much more to do at this point aside from monitoring and waiting for the cluster to recover on its own. Historically, weÕve found that it takes ~15 minutes for the cluster to go from red to yellow.Once the cluster is yellow, we can start to add back indexing-service-web:kubectl scale deployment indexing-service-web --context production --namespace indexing-service --replicas=5# Give a few minutes to make sure the cluster is green...kubectl scale deployment indexing-service-web --context production --namespace indexing-service --replicas=10# ...kubectl scale deployment indexing-service-web --context production --namespace indexing-service --replicas=12In another 20 minutes or so, the cluster should return to green. In the meantime, help with the incident response in #incidents.If the cluster canÕt recoverÉHistorically, the crash has been caused by undetected requests being made to the cluster (e.g., newrelic/newrelic-ruby-agent#2360. As a result, we might want to rollback any recent code changes.If it looks like the master nodes are getting CPU throttling we should work with the Infrastructure team to increase the CPU allocated. For example: salsify/kube-es#149.If the clusterÕs CPU usage seems to be pinned without explanation, we can use the Òhot threadsÓ API to attempt to see what the cluster is running:es_client = Cluster.find(6).clientputs es_client.nodes.hot_threadsIf other options have been exhausted, we could also look to enable HTTP request tracing on the cluster to get visibility into what the cluster is attempting to do:es_client = Cluster.find(6).clientes_client.cluster.put_settings(body: { 'persistent' => { 'logger.org.elasticsearch.http.HttpTracer' => 'TRACE' }})Other Things to ConsiderJob BacklogEven we are unable to get the web pods back up, we might opt to run a handful of indexing-workers (if the cluster can handle it) to avoid having the indexing queue become unmanageable.Dashboards to MonitorIndexing Org Impact for general indexing events and queue count monitoringCluster CPU, memory MonitoringCircuit Breaker Activity Chart - Additional documentation is herePod CPU Quotas Elasticsearch Capacity Planning Sheet to refer to volume and capacity numbersTo check resource-intensive threads on a High CPU node usage, use hot threads API from the rails consoleTo run Bulk Reindex, follow: Running a Bulk Reindex