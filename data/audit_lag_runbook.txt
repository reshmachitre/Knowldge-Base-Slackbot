Audit Lag RunbookThe Go Audit Indexer is a Go deployment that runs as part of the Audits Service that takes events from Kafka, transforms them into a common audit format, and indexes them to Elasticsearch.This runbook describes how to address cases where there are reports of lag in getting those audits indexed.Symptoms* New audits take a long time to show up in the Activity tab (or never show up)* Alerts about consumer group lag for the audits-service-go-audit-indexer consumer groupNoteIf we are getting alerts about the “audits-checker” lagging, we likely need to investigate dropped events: Indexing Validator Crash Looping / "FailedToFindAuditError" RunbookDebuggingCheck the consumer lag in Grafana. If it is continuously increasing, there is likely some fatal issue with the indexer. If the lag is elevated but not increasing, we may need to scale the number of replicas or try to identify the bottleneck in throughputTo verify the indexer is running, look for the go-audit-indexer deployment after running:$ kubectl get pods --context production -n audits-serviceIf things seem to be running, check the logs in New Relic or Bugsnag for errors.Actions to Take* Fix or mitigate bugs that are causing the indexer to crash* Identify bottlenecks to throughputo Was there a surge of events?o Is Elasticsearch hot?o Could we increase the replica count?* Bounce the go audit indexerso We’ve encountered cases where one indexer is deadlocked with assigned partitions, preventing it from making progress. In that case the best thing to do is to bounce the deployment, allowing for fresh indexer to be spun up.Legacy Possible CausesThis section describes so possible causes from previous incidents that might not be relevant to the architecture we have today, but we are keeping them around for posterity and in case they help fix future issues.Reporting DelaysLogstash is very efficient, so if the process is running normally I would not expect it to fall behind even during large spikes of messages. However, the logstash Kafka consumer will commit offsets at regular intervals rather than on every message processed, so it's not uncommon for the lag to spike periodically in between these commits. Prometheus also polls for the Kafka offsets at intervals, so it's possible that Prometheus may see a stretch of large offsets when there is a high volume of messages. These spikes should not last for more than 30 seconds or so.SSL ExpirationIn the past we ran into SSL issues when the Logstash Kafka certificate had expired. These certificates are stored in S3 salsify-audits-service-production bucket and are loaded when the app is deployed. Ops manages these certificates and can update them if needed. The last time this happened, Ops changed the expiration date to 10 years in the future, so if you reading this in 2030 this might be the culprit.Rebalancing LoopSometimes Logstash gets stuck rebalancing between consumers in the group. There are currently 3 Logstash processes, and each process has 64 worker threads. Each worker thread runs its own consumer. If any of the consumers fail to send a heartbeat every 10 seconds, they will be removed from the consumer group and all of the other consumers need to rebalance. The rebalancing will invalidate any work the current consumers are doing, so if consumers are continuously rebalancing they won’t make much progress.Normally it shouldn’t be a problem to heartbeat every 10 seconds, but we’ve seen cases where some threads become unresponsive. This writeup from an incident has some more details.Eventually the consumers tend to settle out, but this can take a while. One hypothesis is that some Avro messages take a long time to decode, and the consumers decode batches of 500 messages at once. If it takes 10 seconds or longer to decode this batch of messages, it could miss the heartbeat interval and the Kafka broker will kick it out of the group. To test this out, we should try reducing the max.poll.records amount from 500 to ~100 or so and see if that fixes it. This is a setting for the Logstash plugin: https://www.elastic.co/guide/en/logstash/6.8/plugins-inputs-kafka.html#plugins-inputs-kafka-max_poll_recordsArchiver LagThe Audits Service archiver process is a Kafka consumer that reads messages from a Kafka topic and buffers them in Avro datafiles. When the files hit a certain size (~50MB) or have been open for a while (> 15 minutes), it will close the file, upload it to S3, and then commit the offsets processed to Kafka. These files in S3 are fed into the Data Lake and can also be used for offline analysis.Because the archiver does not commit offsets until it has written the file to S3, the offset lag will grow until it hits a certain size or age and then shrink back down close to zero (the chart of the consumer lag for this group should have a sawtooth pattern). This alert is fired if the consumer lag is above a threshold for more than hour.The archived events are not used to power any realtime processes, but if we don't address the issue within Kafka's data retention period, we risk data loss once the messages are expired from Kafka.DebuggingThe archiver is efficient and a single process can easily handle all messages on a topic. If the consumer group offset lag for the archiver continues to grow without shrinking at regular interviews, the process is most likely malfunctioning. In the past we have run into memory issues killing the process, so check Kubernetes that the process hasn't been restarted from OOM errors.There have also been SSL connection errors in the past due to expired certificates. See the “SSL Expiration” section above for more details.Integration Test FailureWe run an integration test in production once an hour via a Kubernetes cronjob. This test makes a random change in a production organization and then queries the audits service until it's available. If the change is not available within 10 minutes or so, it will generate a Bugsnag and the job will fail.If this happens, check the logs to see where the error is occurring. Occasionally, there is a 500 error from Dandelion while making the product update. If the product update went through, check that the audit indexer isn't lagging behind. If indexing is working fine, it can mean there is a delay publishing events out of Dandelion.NoteOnce the job has failed, you have to manually clear it out of Kubernetes. Run kubectl get jobs to find the failed job (it should have 0/1 completions) and then manually delete the job with kubectl delete job.